{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a97a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import set_tracing_export_api_key\n",
    "tracing_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "set_tracing_export_api_key(tracing_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5516b70",
   "metadata": {},
   "source": [
    "Hello world example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d80f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "\n",
    "agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n",
    "\n",
    "# Intended for Jupyter notebooks where there's an existing event loop\n",
    "result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")  # type: ignore[top-level-await]  # noqa: F704\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e11bd",
   "metadata": {},
   "source": [
    "Handoffs example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner\n",
    "import asyncio\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"Spanish agent\",\n",
    "    instructions=\"You only speak Spanish.\",\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"English agent\",\n",
    "    instructions=\"You only speak English\",\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[spanish_agent, english_agent],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n",
    "print(result.final_output)\n",
    "    # ¡Hola! Estoy bien, gracias por preguntar. ¿Y tú, cómo estás?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd7787",
   "metadata": {},
   "source": [
    "Functions example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents import Agent, Runner, function_tool\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def get_weather(city: str) -> str:\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Hello world\",\n",
    "    instructions=\"You are a helpful agent.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    tools=[get_weather],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result = await Runner.run(agent, input=\"What's the weather in Tokyo?\")\n",
    "print(result.final_output)\n",
    "    # The weather in Tokyo is sunny.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9628c",
   "metadata": {},
   "source": [
    "Deterministic Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2ff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import Agent, Runner, trace\n",
    "\n",
    "\"\"\"\n",
    "This example demonstrates a deterministic flow, where each step is performed by an agent.\n",
    "1. The first agent generates a story outline\n",
    "2. We feed the outline into the second agent\n",
    "3. The second agent checks if the outline is good quality and if it is a scifi story\n",
    "4. If the outline is not good quality or not a scifi story, we stop here\n",
    "5. If the outline is good quality and a scifi story, we feed the outline into the third agent\n",
    "6. The third agent writes the story\n",
    "\"\"\"\n",
    "\n",
    "story_outline_agent = Agent(\n",
    "    name=\"story_outline_agent\",\n",
    "    instructions=\"Generate a very short story outline based on the user's input.\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "class OutlineCheckerOutput(BaseModel):\n",
    "    good_quality: bool\n",
    "    is_scifi: bool\n",
    "\n",
    "\n",
    "outline_checker_agent = Agent(\n",
    "    name=\"outline_checker_agent\",\n",
    "    instructions=\"Read the given story outline, and judge the quality. Also, determine if it is a scifi story.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    output_type=OutlineCheckerOutput,\n",
    ")\n",
    "\n",
    "story_agent = Agent(\n",
    "    name=\"story_agent\",\n",
    "    instructions=\"Write a short story based on the given outline.\",\n",
    "    output_type=str,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "input_prompt = input(\"What kind of story do you want? \")\n",
    "\n",
    "# Ensure the entire workflow is a single trace\n",
    "with trace(\"Deterministic story flow\"):\n",
    "    # 1. Generate an outline\n",
    "    outline_result = await Runner.run(\n",
    "        story_outline_agent,\n",
    "        input_prompt,\n",
    "    )\n",
    "    print(\"Outline generated\")\n",
    "\n",
    "    # 2. Check the outline\n",
    "    outline_checker_result = await Runner.run(\n",
    "        outline_checker_agent,\n",
    "        outline_result.final_output,\n",
    "    )\n",
    "\n",
    "    print(outline_checker_result.final_output)\n",
    "    # 3. Add a gate to stop if the outline is not good quality or not a scifi story\n",
    "    assert isinstance(outline_checker_result.final_output, OutlineCheckerOutput)\n",
    "    if not outline_checker_result.final_output.good_quality:\n",
    "        print(\"Outline is not good quality, so we stop here.\")\n",
    "        exit(0)\n",
    "\n",
    "    if not outline_checker_result.final_output.is_scifi:\n",
    "        print(\"Outline is not a scifi story, so we stop here.\")\n",
    "        exit(0)\n",
    "\n",
    "    print(\"Outline is good quality and a scifi story, so we continue to write the story.\")\n",
    "\n",
    "    # 4. Write the story\n",
    "    story_result = await Runner.run(\n",
    "        story_agent,\n",
    "        outline_result.final_output,\n",
    "    )\n",
    "    print(f\"Story: {story_result.final_output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd3d56a",
   "metadata": {},
   "source": [
    "Agents as tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44409919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, ItemHelpers, MessageOutputItem, Runner, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows the agents-as-tools pattern. The frontline agent receives a user message and\n",
    "then picks which agents to call, as tools. In this case, it picks from a set of translation\n",
    "agents.\n",
    "\"\"\"\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You translate the user's message to Spanish\",\n",
    "    handoff_description=\"An english to spanish translator\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions=\"You translate the user's message to French\",\n",
    "    handoff_description=\"An english to french translator\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "italian_agent = Agent(\n",
    "    name=\"italian_agent\",\n",
    "    instructions=\"You translate the user's message to Italian\",\n",
    "    handoff_description=\"An english to italian translator\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"orchestrator_agent\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    instructions=(\n",
    "        \"You are a translation agent. You use the tools given to you to translate.\"\n",
    "        \"If asked for multiple translations, you call the relevant tools in order.\"\n",
    "        \"You never translate on your own, you always use the provided tools.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        spanish_agent.as_tool(\n",
    "            tool_name=\"translate_to_spanish\",\n",
    "            tool_description=\"Translate the user's message to Spanish\",\n",
    "        ),\n",
    "        french_agent.as_tool(\n",
    "            tool_name=\"translate_to_french\",\n",
    "            tool_description=\"Translate the user's message to French\",\n",
    "        ),\n",
    "        italian_agent.as_tool(\n",
    "            tool_name=\"translate_to_italian\",\n",
    "            tool_description=\"Translate the user's message to Italian\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "synthesizer_agent = Agent(\n",
    "    name=\"synthesizer_agent\",\n",
    "    instructions=\"You inspect translations, correct them if needed, and produce a final concatenated response.\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "msg = input(\"Hi! What would you like translated, and to which languages? \")\n",
    "\n",
    "# Run the entire orchestration in a single trace\n",
    "with trace(\"Orchestrator evaluator\"):\n",
    "    orchestrator_result = await Runner.run(orchestrator_agent, msg)\n",
    "\n",
    "    for item in orchestrator_result.new_items:\n",
    "        if isinstance(item, MessageOutputItem):\n",
    "            text = ItemHelpers.text_message_output(item)\n",
    "            if text:\n",
    "                print(f\"  - Translation step: {text}\")\n",
    "\n",
    "    synthesizer_result = await Runner.run(\n",
    "        synthesizer_agent, orchestrator_result.to_input_list()\n",
    "    )\n",
    "\n",
    "print(f\"\\n\\nFinal response:\\n{synthesizer_result.final_output}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529f7b4",
   "metadata": {},
   "source": [
    "LLM as judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows the LLM as a judge pattern. The first agent generates an outline for a story.\n",
    "The second agent judges the outline and provides feedback. We loop until the judge is satisfied\n",
    "with the outline.\n",
    "\"\"\"\n",
    "\n",
    "story_outline_generator = Agent(\n",
    "    name=\"story_outline_generator\",\n",
    "    instructions=(\n",
    "        \"You generate a very short story outline based on the user's input. \"\n",
    "        \"If there is any feedback provided, use it to improve the outline.\"\n",
    "    ),\n",
    "    model=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvaluationFeedback:\n",
    "    feedback: str\n",
    "    score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n",
    "\n",
    "\n",
    "evaluator = Agent[None](\n",
    "    name=\"evaluator\",\n",
    "    instructions=(\n",
    "        \"You evaluate a story outline and decide if it's good enough. \"\n",
    "        \"If it's not good enough, you provide feedback on what needs to be improved. \"\n",
    "        \"Never give it a pass on the first try. After 5 attempts, you can give it a pass if the story outline is good enough - do not go for perfection\"\n",
    "    ),\n",
    "    output_type=EvaluationFeedback,\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "msg = input(\"What kind of story would you like to hear? \")\n",
    "input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "latest_outline: str | None = None\n",
    "i = 1\n",
    "# We'll run the entire workflow in a single trace\n",
    "with trace(\"LLM as a judge\"):\n",
    "    while i <= 2:\n",
    "        story_outline_result = await Runner.run(\n",
    "            story_outline_generator,\n",
    "            input_items,\n",
    "        )\n",
    "\n",
    "        input_items = story_outline_result.to_input_list()\n",
    "        print(f\"input items {input_items}\")\n",
    "        latest_outline = ItemHelpers.text_message_outputs(story_outline_result.new_items)\n",
    "        print(\"Story outline generated\")\n",
    "\n",
    "        evaluator_result = await Runner.run(evaluator, input_items)\n",
    "        result: EvaluationFeedback = evaluator_result.final_output\n",
    "\n",
    "        print(f\"Evaluator score: {result.score}\")\n",
    "\n",
    "        if result.score == \"pass\":\n",
    "            print(\"Story outline is good enough, exiting.\")\n",
    "            break\n",
    "\n",
    "        print(\"Re-running with feedback\")\n",
    "        i +=1 \n",
    "        input_items.append({\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})\n",
    "\n",
    "print(f\"Final story outline: {latest_outline}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a918924",
   "metadata": {},
   "source": [
    "Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, trace\n",
    "\n",
    "\"\"\"\n",
    "This example shows the parallelization pattern. We run the agent three times in parallel, and pick\n",
    "the best result.\n",
    "\"\"\"\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You translate the user's message to Spanish\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "translation_picker = Agent(\n",
    "    name=\"translation_picker\",\n",
    "    instructions=\"You pick the best Spanish translation from the given options.\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "msg = input(\"Hi! Enter a message, and we'll translate it to Spanish.\\n\\n\")\n",
    "print(msg)\n",
    "# Ensure the entire workflow is a single trace\n",
    "with trace(\"Parallel translation\"):\n",
    "    res_1, res_2, res_3 = await asyncio.gather(\n",
    "        Runner.run(\n",
    "            spanish_agent,\n",
    "            msg,\n",
    "        ),\n",
    "        Runner.run(\n",
    "            spanish_agent,\n",
    "            msg,\n",
    "        ),\n",
    "        Runner.run(\n",
    "            spanish_agent,\n",
    "            msg,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    outputs = [\n",
    "        ItemHelpers.text_message_outputs(res_1.new_items),\n",
    "        ItemHelpers.text_message_outputs(res_2.new_items),\n",
    "        ItemHelpers.text_message_outputs(res_3.new_items),\n",
    "    ]\n",
    "\n",
    "    translations = \"\\n\\n\".join(outputs)\n",
    "    print(f\"\\n\\nTranslations:\\n\\n{translations}\")\n",
    "\n",
    "    best_translation = await Runner.run(\n",
    "        translation_picker,\n",
    "        f\"Input: {msg}\\n\\nTranslations:\\n{translations}\",\n",
    "    )\n",
    "\n",
    "print(\"\\n\\n-----\")\n",
    "\n",
    "print(f\"Best translation: {best_translation.final_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d54e2cb",
   "metadata": {},
   "source": [
    "Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e74e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    InputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    TResponseInputItem,\n",
    "    input_guardrail,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to use guardrails.\n",
    "\n",
    "Guardrails are checks that run in parallel to the agent's execution.\n",
    "They can be used to do things like:\n",
    "- Check if input messages are off-topic\n",
    "- Check that input messages don't violate any policies\n",
    "- Take over control of the agent's execution if an unexpected input is detected\n",
    "\n",
    "In this example, we'll setup an input guardrail that trips if the user is asking to do math homework.\n",
    "If the guardrail trips, we'll respond with a refusal message.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "### 1. An agent-based guardrail that is triggered if the user is asking to do math homework\n",
    "class MathHomeworkOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    is_math_homework: bool\n",
    "\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking you to do their math homework.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    output_type=MathHomeworkOutput,\n",
    ")\n",
    "\n",
    "\n",
    "@input_guardrail\n",
    "async def math_guardrail(\n",
    "    context: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n",
    ") -> GuardrailFunctionOutput:\n",
    "    \"\"\"This is an input guardrail function, which happens to call an agent to check if the input\n",
    "    is a math homework question.\n",
    "    \"\"\"\n",
    "    result = await Runner.run(guardrail_agent, input, context=context.context)\n",
    "    final_output = result.final_output_as(MathHomeworkOutput)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,\n",
    "        tripwire_triggered=final_output.is_math_homework,\n",
    "    )\n",
    "\n",
    "\n",
    "### 2. The run loop\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Customer support agent\",\n",
    "    instructions=\"You are a customer support agent. You help customers with their questions.\",\n",
    "    model=\"gpt-5-mini\",\n",
    "    input_guardrails=[math_guardrail],\n",
    ")\n",
    "\n",
    "input_data: list[TResponseInputItem] = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter a message: \")\n",
    "    input_data.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_input,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = await Runner.run(agent, input_data)\n",
    "        print(result.final_output)\n",
    "        # If the guardrail didn't trigger, we use the result as the input for the next run\n",
    "        input_data = result.to_input_list()\n",
    "    except InputGuardrailTripwireTriggered:\n",
    "        # If the guardrail triggered, we instead add a refusal message to the input\n",
    "        message = \"Sorry, I can't help you with your math homework.\"\n",
    "        print(message)\n",
    "        input_data.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Sample run:\n",
    "# Enter a message: What's the capital of California?\n",
    "# The capital of California is Sacramento.\n",
    "# Enter a message: Can you help me solve for x: 2x + 5 = 11\n",
    "# Sorry, I can't help you with your math homework.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b892f",
   "metadata": {},
   "source": [
    "Output guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2acbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    output_guardrail,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to use output guardrails.\n",
    "\n",
    "Output guardrails are checks that run on the final output of an agent.\n",
    "They can be used to do things like:\n",
    "- Check if the output contains sensitive data\n",
    "- Check if the output is a valid response to the user's message\n",
    "\n",
    "In this example, we'll use a (contrived) example where we check if the agent's response contains\n",
    "a phone number.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# The agent's output type\n",
    "class MessageOutput(BaseModel):\n",
    "    reasoning: str = Field(description=\"Thoughts on how to respond to the user's message\")\n",
    "    response: str = Field(description=\"The response to the user's message\")\n",
    "    user_name: str | None = Field(description=\"The name of the user who sent the message, if known\")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def sensitive_data_check(\n",
    "    context: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    phone_number_in_response = \"650\" in output.response\n",
    "    phone_number_in_reasoning = \"650\" in output.reasoning\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\n",
    "            \"phone_number_in_response\": phone_number_in_response,\n",
    "            \"phone_number_in_reasoning\": phone_number_in_reasoning,\n",
    "        },\n",
    "        tripwire_triggered=phone_number_in_response or phone_number_in_reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    output_type=MessageOutput,\n",
    "    output_guardrails=[sensitive_data_check],\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "    # This should be ok\n",
    "result = await Runner.run(agent, \"What's the capital of California?\")\n",
    "print(\"First message passed\")\n",
    "print(result.final_output)\n",
    "\n",
    "# This should trip the guardrail\n",
    "try:\n",
    "    result = await Runner.run(\n",
    "        agent, \"My phone number is 650-123-4567. Where do you think I live?\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Guardrail didn't trip - this is unexpected. Output: {json.dumps(result.final_output.model_dump(), indent=2)}\"\n",
    "    )\n",
    "\n",
    "except OutputGuardrailTripwireTriggered as e:\n",
    "    print(f\"Guardrail tripped. Info: {e.guardrail_result.output.output_info}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75addad",
   "metadata": {},
   "source": [
    "Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, SQLiteSession, trace\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"Reply very concisely.\",\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "# Create a session instance\n",
    "session = SQLiteSession(\"conversation_123\")\n",
    "\n",
    "with trace(\"SQL Lite session\"):\n",
    "    # First turn\n",
    "    result = await Runner.run(\n",
    "        agent,\n",
    "        \"What city is the Golden Gate Bridge in?\" ,\n",
    "        session=session\n",
    "    )\n",
    "    print(result.final_output)  # \"San Francisco\"\n",
    "\n",
    "    # Second turn - agent automatically remembers previous context\n",
    "    result = await Runner.run(\n",
    "        agent,\n",
    "        \"What state is it in?\",\n",
    "        session=session\n",
    "    )\n",
    "    print(result.final_output)  # \"California\"\n",
    "\n",
    "    # Also works with synchronous runner\n",
    "    result = await Runner.run(\n",
    "        agent,\n",
    "        \"What's the population?\",\n",
    "        session=session\n",
    "    )\n",
    "    print(result.final_output)  # \"Approximately 39 million\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeec772",
   "metadata": {},
   "source": [
    "LLM as a judge with sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0221d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "\n",
    "from agents import Agent, ItemHelpers, Runner, TResponseInputItem, trace, SQLiteSession, RunConfig,Session\n",
    "\n",
    "\"\"\"\n",
    "This example shows the LLM as a judge pattern. The first agent generates an outline for a story.\n",
    "The second agent judges the outline and provides feedback. We loop until the judge is satisfied\n",
    "with the outline.\n",
    "\"\"\"\n",
    "\n",
    "story_outline_generator = Agent(\n",
    "    name=\"story_outline_generator\",\n",
    "    instructions=(\n",
    "        \"You generate a very short story outline based on the user's input. \"\n",
    "        \"If there is any feedback provided, use it to improve the outline.\"\n",
    "    ),\n",
    "    model=\"gpt-5-nano\"\n",
    ")\n",
    "\n",
    "\n",
    "# def session_input_callback(history_items, new_items):\n",
    "#     print(f\"history: {history_items}\")\n",
    "#     print(f\"new iems: {new_items}\")\n",
    "\n",
    "# run_config=RunConfig(\n",
    "#             session_input_callback= session_input_callback,\n",
    "#         ),\n",
    "\n",
    "@dataclass\n",
    "class EvaluationFeedback:\n",
    "    feedback: str\n",
    "    score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n",
    "\n",
    "\n",
    "evaluator = Agent[None](\n",
    "    name=\"evaluator\",\n",
    "    instructions=(\n",
    "        \"You evaluate a story outline and decide if it's good enough. \"\n",
    "        \"If it's not good enough, you provide feedback on what needs to be improved. \"\n",
    "        \"Never give it a pass on the first try. After 5 attempts, you can give it a pass if the story outline is good enough - do not go for perfection\"\n",
    "    ),\n",
    "    output_type=EvaluationFeedback,\n",
    "    model=\"gpt-5-mini\"\n",
    ")\n",
    "\n",
    "# Create a session instance\n",
    "session = SQLiteSession(\"conversation_1335\")\n",
    "print(f\"session: {await session.get_items()}\")\n",
    "\n",
    "msg = input(\"What kind of story would you like to hear? \")\n",
    "input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "latest_outline: str | None = None\n",
    "i = 1\n",
    "# We'll run the entire workflow in a single trace\n",
    "with trace(\"LLM as a judge with sessions\"):\n",
    "    while i <= 1:\n",
    "        story_outline_result = await Runner.run(\n",
    "            story_outline_generator,\n",
    "            input_items,\n",
    "            session=session\n",
    "        )\n",
    "\n",
    "        print(f\"story_outline_result: {story_outline_result}\")\n",
    "        latest_outline = ItemHelpers.text_message_outputs(story_outline_result.new_items)\n",
    "        print(f\"Story outline generated: {latest_outline}\")\n",
    "        print(f\"session: {await session.get_items()}\")\n",
    "\n",
    "        # evaluator_result = await Runner.run(\n",
    "        #     evaluator, \n",
    "        #     latest_outline, \n",
    "        #     session=session)\n",
    "        # result: EvaluationFeedback = evaluator_result.final_output\n",
    "\n",
    "        # print(f\"evaluator_result: {evaluator_result}\")\n",
    "\n",
    "        # print(f\"Evaluator score: {result.score}\")\n",
    "\n",
    "        # print(f\"session: {await session.get_items()}\")\n",
    "        # if result.score == \"pass\":\n",
    "        #    print(\"Story outline is good enough, exiting.\")\n",
    "        #    break\n",
    "\n",
    "        print(\"Re-running with feedback\")\n",
    "        i +=1\n",
    "\n",
    "print(f\"Final story outline: {latest_outline}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
